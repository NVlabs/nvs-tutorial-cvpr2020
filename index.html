<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html>

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Novel View Synthesis: From Depth-Based Warping to Multi-Plane Images and Beyond</title>
  <meta name="author" content="Orazio Gallo">
  <meta name="keywords" content="deep learning, tutorial, novel view synthesis, Multi-Plane imaging, ">
  <link rel="stylesheet" href="style.css">
  <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Open+Sans" />
</head>



<body>

  <div align="center">
    
    <div class="titlesection">
      <h2>CVPR 2020 Tutorial on</h2>
      <h1>Novel View Synthesis: From Depth-Based Warping to Multi-Plane Images and Beyond</h1>
      <video loop="" autoplay="" muted="" height="167" ">
        <source src="imgs/jump.mp4" type="video/mp4">
        </video>
        <video loop="" autoplay="" muted="" height="167" ">
          <source src="https://storage.googleapis.com/nerf_data/website_renders/orchid.mp4" type="video/mp4">
          </video>
          <img src="imgs/synsin.gif" height="167">
      <video loop="" autoplay="" muted="" height="167" ">
        <source src="imgs/evs.mp4" type="video/mp4">
        </video>
      </div>
      
      <div class="textsection">
        Novel view synthesis is a long-standing problem at the intersection of computer graphics and computer vision.
        Seminal work in this field dates back to the 1990s, with early methods proposing to interpolate either between corresponding pixels from the input images, or between rays in space.
        Recent deep learning methods enabled tremendous improvements to the quality of the results, and brought renewed popularity to the field.
        The teaser above shows novel view synthesis from different recent methods. <i>From left to right: Yoon et al. [1], Mildenhall et al. [2], Wiles et al. [3], and Choi et al. [4]. Images and videos courtesy of the respective authors.</i>
      </div>
      
    <div class="titlesection">
      <h3 style="color:red"> >>> The tutorial will be live <a href="https://youtu.be/OEUHalxanuc", target="_blank">here</a>. <<<</h3>
      To ask questions email orazio.gallo@gmail.com or type them in the YouTube chat.
      <!-- (need to go to the <a href="https://youtu.be/OEUHalxanuc" , target="_blank">YouTube page of the video</a>) -->
    </div>

    <!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/OEUHalxanuc" frameborder="0"
      allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
    
    <div class="textsection">
      <h3>Goal of the Tutorial</h3>
      In this tutorial we will first introduce the problem, including offering context and a taxonomy of the different methods. We will then have talks by the researchers behind the most recent approaches in the field.
      </br></br>
      At the end of the tutorial we will have a roundtable discussion with all the speakers.
      If you have questions for the roundtable discussion, please send them to <a href="mailto:ogallo@nvidia.com">Orazio</a>.
    </div>
    
    
    <div class="textsection">
      <h3>Date and Location</h3>
      The tutorial will take place on June 14th, 2020.
      The current plan is to have live presentations and to use Youtube to cast the content live.
      The link to the live cast will be posted here.
    </div>
    
    
    <h3>Organizers</h3>
    <div id="container_organizers">
      <div class="box">
        <img class="circularImage" src="imgs/orazio_s.jpeg" border="1" width="150" alt="Orazio's pic"></br>
        Orazio Gallo <a href="https://twitter.com/0razio?ref_src=twsrc%5Etfw"><img src="imgs/twitter.jpeg" height="19"></a></br>
        NVIDIA
      </div>
      <div class="box">
        <img class="circularImage" src="imgs/alejandro_s.jpeg"  border="1" width="150" alt="Alejandro's pic"></br>
        <a href="https://research.nvidia.com/person/alejandro-troccoli">Alejandro Troccoli</a></br>
        NVIDIA
      </div>
      <div class="box">
        <img class="circularImage" src="imgs/varun_s.jpeg"  border="1" width="150" alt="Varun's pic"></br>
        <a href="https://varunjampani.github.io">Varun Jampani</a></br>
        Google
      </div>
      <span class="stretch"></span>
    </div>
    
    
    
    <h3>Invited Speakers</h3>
    <div id="container_speakers">
      <div class="box">
        <img class="circularImage" src="imgs/rick_s.jpg" border="1" width="150" alt="Rick's pic"></br>
        <a href="http://szeliski.org/RichardSzeliski.htm">Rick Szeliski</a></br>
        Facebook
      </div>
      
      <div class="box">
        <img class="circularImage" src="imgs/pratul_s.jpg"  border="1" width="150" alt="Pratul's pic"></br>
        <a href="https://people.eecs.berkeley.edu/~pratul/">Pratul Srinivasan</a></br>
        UC Berkeley
      </div>
      
      <div class="box">
        <img class="circularImage" src="imgs/richard_s2.jpg"  border="1" width="150" alt="Richard's pic"></br>
        Richard Tucker</br>
        Google
      </div>
      
        <div class="box">
          <img class="circularImage" src="imgs/olivia_s.jpg" border="1" width="150" alt="Olivia's pic"></br>
          <a href="http://www.robots.ox.ac.uk/~ow/">Olivia Wiles</a></br>
          University of Oxford
        </div>
        
        <span class="stretch"></span>
    </div>
    
    <div id="container_speakers_3">
      <div class="box">
        <img class="circularImage" src="imgs/jaeshin_s.jpg" border="1" width="150" alt="Jae Shin's pic"></br>
        <a href="https://www-users.cs.umn.edu/~jsyoon/">Jae Shin Yoon</a></br>
        UMN
      </div>
      
      <div class="box">
        <img class="circularImage" src="imgs/gaurav_s.jpg" border="1" width="150" alt="Gaurav's pic"></br>
        <a href="https://gchauras.github.io/research/">Gaurav Chaurasia</a></br>
        Oculus
      </div>
      
      <div class="box">
        <img class="circularImage" src="imgs/nima_s.jpg"  border="1" width="150" alt="Nima's pic"></br>
        <a href="http://faculty.cs.tamu.edu/nimak/">Nima Kalantari</a></br>
        Texas A&M
      </div>
      <span class="stretch"></span>
    </div>

    <div class="textsection">
      <div align="center">
        <span>
        <h2>Tentative Program</h2>
        <p>
          <table style="width:95%">
            <td></td>
            <td bgcolor="#DDDDDD">Talk Title</td>
            <td bgcolor="#DDDDDD">Speaker</td>
          </tr>
        
        <tr>
          <td bgcolor="#DDDDDD">9:20 - 9:50</td>
          <td>
            Novel View Synthesis: A Gentle Introduction
          </td>
          <td>Organizers</td>
          </tr>
          
          <tr>
            <td bgcolor="#DDDDDD">9:50 - 10:20</td>
            <td>
              Lumigraphs and Reflections
            </td>
            <td>Rick</td>
          </tr>
          
          <tr>
            <td bgcolor="#DDDDDD">10:20 - 10:50</td>
            <td>
              SynSin: Single Image View Synthesis
            </td>
            <td>Olivia</td>
          </tr>
          
          <tr>
            <td>10:50 - 11:00</td>
            <th>Coffee break (15m)</th>
            <td></td>
          </tr>
          
          <tr>
            <td bgcolor="#DDDDDD">11:00 - 11:30</td>
            <td>
              View synthesis with Multiplane Images
            </td>
            <td>Richard</td>
          </tr>
          
          <tr>
            <td bgcolor="#DDDDDD">11:30 - 12:00</td>
            <td>
              View Synthesis and Immersive Mixed Reality for VR devices
            </td>
            <td>Gaurav</td>
          </tr>
          
          <tr>
            <td>12:00 - 12:45</td>
            <!-- <th colspan="2">Lunch break (45m)</th> -->
            <th>Lunch break (45m)</th>
            <td></td>
          </tr>
          
          <tr>
            <td bgcolor="#DDDDDD">12:45 - 13:15</td>
            <td>
              View and Frame Interpolation for Consumer Light Field Cameras
            </td>
            <td>Nima</td>
          </tr>
          
          <tr>
            <td bgcolor="#DDDDDD">13:15 - 13:45</td>
            <td>
              NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis
            </td>
            <td>Pratul</td>
          </tr>
          
          <tr>
            <td bgcolor="#DDDDDD">13:45 - 14:15</td>
            <td>
              Novel View Synthesis from Dynamic Scenes
	    </td>
            <td>Jae Shin</td>
          </tr>
          
          <tr>
            <td>14:15 - 14:30</td>
            <th>Coffee break (15m)</th>
            <td></td>
          </tr>
          
          <tr>
            <td bgcolor="#DDDDDD">14:30 - 15:30</td>
            <td>Round Table Discussion With the Invited Speakers</td>
            <td></td>
          </tr>
          
        </table>
      </p>
    </span>
  </div>
</div>

<div align="center">
<a href="mailto:ogallo@nvidia.com">Contact Us</a>
</div>
  <div class="textsection">
    
    
    <h2>References</h2>
    [1] Yoon, Kim, Gallo, Park, and Kautz, "Novel View Synthesis of Dynamic Scenes with Globally
    Coherent Depths from a Monocular Camera" IEEE CVPR 2020.</br>
    [2] Mildenhall, Srinivasan, Tancik, Barron,  Ramamoorthi, and Ng, "NeRF: Representing Scenes
    as Neural Radiance Fields for View Synthesis" arXiv 2020.</br>
    [3] Wiles, Gkioxari, Szeliski, and Johnson, "SynSin: End-to-end View Synthesis from a Single Image"
    IEEE CVPR 2020.</br>
    [4] Choi, Gallo, Troccoli, Kim, and Kautz, "Extreme view synthesis" IEEE
    ICCV 2019.</br>
    
  </div> 
  
  
</body>

</html>
